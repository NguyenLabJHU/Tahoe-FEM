%&LaTeX
% $Id: execution.tex,v 1.4 2001-04-23 05:07:41 paklein Exp $

\section{Execution procedures}
\tahoe has be ported to a number of platforms.  The code can be executed in
parallel as well as in serial.  Furthermore, \tahoe runs in several modes
that have been added to support execution in parallel.  These modes are
listed below.  Only the Parallel Analysis mode actually runs in parallel,
the other modes run in serial.
\begin{itemize}
\item[(\ref{sect.serial.analysis})] Serial analysis
\item[(\ref{sect.parallel.analysis})] Parallel analysis
\item[(\ref{sect.domain.decomp})] Domain decomposition
\item[(\ref{sect.joining})] Joining results data
\end{itemize}
The procedure for running in serial and parallel has been made as similar
as possible.  The only differences arise from machine-dependent
limitations.  The parameters file and geometry file that are needed to
define an analysis are identical for both modes of analysis.  Some
additional files are needed to run parallel analysis, but \tahoe generates
these for the user.  The user must only be aware of which files are needed
on the machine for each mode of operation at the time of execution.  The
following sections explain how to execute \tahoe with the following command
line options:
\begin{center}
\begin{tabular}[c]{c c}
\parbox[b]{2.0in}{\raggedleft \texttt{tahoe}} &\parbox[b]{2.0in}{}\\
&\parbox[b]{2.0in}{\texttt{-f} $\sbrkt{\textit{job name}}$\texttt{.in}}\\
&\parbox[b]{2.0in}{\texttt{-decomp}}\\
&\parbox[b]{2.0in}{\texttt{-join}}\\
&\parbox[b]{2.0in}{\texttt{-split\_io}}\\
\end{tabular}
\end{center}

\subsection{Serial analysis}
\label{sect.serial.analysis}
For running serially, the user must have:
\begin{center}
\begin{tabular}[c]{c c}
\parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{job name}}$\texttt{.in}} 
&\parbox[b]{3.0in}{parameters file}\\
\parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{geometry name}}$\texttt{.exo}} 
&\parbox[b]{3.0in}{geometry file (\textsf{ExodusII}~\cite{ExodusII})}\\
\end{tabular}
\end{center}
The file extensions are not interpreted by \tahoe, so these may be selected
arbitrarily.  The resulting output files will be:
\begin{center}
\begin{tabular}[c]{c c}
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{job name}}$\texttt{.out}} 
&\parbox[b]{3.0in}{``echo'' file of parameters file}\\
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{job name}}$\texttt{.io}$\sbrkt{\textit{j}}$\texttt{.exo}} 
&\parbox[b]{3.0in}{results data (\textsf{ExodusII}~\cite{ExodusII})}\\
\end{tabular}
\end{center}
The results data have labels $j$,  
where $j = 0,\ldots,\rbrkt{\textit{number of output groups} - 1}$.
The number of output groups is determined by the type of analysis. 
For the simplest case, each element group will produce one output group,
and the sequence of output groups will be the same as the sequence the
element groups are declared in the parameters file.  Each results file has
only one element block corresponding to the connectivities for the element
group.  If multiple element blocks are used to define the connectivities
for a given element group, these will be combined into a single element
block for output.  Serial analysis can also be done by running a parallel
executable with the number of processors specified as one.

\subsection{Parallel analysis}
\label{sect.parallel.analysis}
The procedure for launching parallel executables varies depending on the
user environment. In the examples that follow, we assume parallel
executables are launched using the \texttt{mpirun} command:
\begin{center}
	\texttt{\% mpirun -np} $\sbrkt{\textit{number of processors}}$ \texttt{tahoe}
\end{center}
Users should substitute the appropriate command, such as \texttt{dmpirun} 
or \texttt{yod}, depending on the environment.

The procedure for running \tahoe in parallel varies depending on whether the
machine has ``limited'' memory with respect to the problem size.  A machine
is considered to have ``limited'' memory with respect to a given problem if
the entire model from the geometry definition cannot be stored in memory at
the same time.  On machines that are not memory limited, execution of \tahoe
is \emph{identical} whether running on serial or in parallel, aside from any
special procedure needed to launch a parallel job.  For this case, no
additional steps are required for the user in order to pre- and post-process 
the model definition.  With limited memory machines, a non-limited
machine is needed in order to decompose the model geometry and join the
results data after execution.  Domain decomposition and result file joining
is discussed in the next two sections.

The identical input files (parameters and geometry) are needed to run in
parallel as are needed to run in serial.
\begin{center}
\begin{tabular}[c]{c c}
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{job name}}$\texttt{.in}}
&\parbox[b]{3.0in}{parameters file}\\
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{geometry name}}$\texttt{.exo}}
&\parbox[b]{3.0in}{geometry file (\textsf{ExodusII}~\cite{ExodusII})}\\
\end{tabular}
\end{center}
The first thing that the program does is look to see if the required
decomposition files already exist.  \tahoe creates a domain decomposition
based on the connectivities seen at run time.  If the geometry file
contains blocks of elements that are not referenced in the parameters file,
these blocks are not considered during decomposition.  Additionally,
meshfree methods generate connectivities at run time the depend on
parameters defining the shape functions.  \tahoe does not keep track of
which values in the parameters file affect the domain decomposition.  If
any of these parameters are changed, the user must delete any of the
corresponding decomposition files.  The decomposition files are:
\begin{center}
\begin{tabular}[c]{c c}
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{geometry name}}$\texttt{.io.exo}}
&\parbox[b]{3.0in}{global geometry file}\\
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{geometry name}}$\texttt{.n}$\sbrkt{\textit{M}}$\texttt{.io.map}}
&\parbox[b]{3.0in}{output map}\\
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{geometry name}}$\texttt{.io.ID}}
&\parbox[b]{3.0in}{block ID's per output group}\\
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{geometry name}}$\texttt{.n}$\sbrkt{{M}}$\texttt{.part}$\sbrkt{\textit{i}}$}
&\parbox[b]{3.0in}{decomposition data file}\\
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{geometry name}}$\texttt{.n}$\sbrkt{\textit{M}}$\texttt{.p}$\sbrkt{\textit{i}}$\texttt{.exo}}
&\parbox[b]{3.0in}{partial geometry file}
\end{tabular}
\end{center}
In the file names above, $M$ is the total number of partitions, 
while $i = 0,\ldots,\rbrkt{M - 1}$.
If any decomposition files are not found, \tahoe will attempt
to create all of them.  If \tahoe is launched on a non-limited memory
machine, mesh decomposition is done serially on the processor with rank 0
before starting the analysis phase of execution in parallel.  If the
decomposition files already exist, the decomposition phase is not repeated. 
\tahoe produces the following files during execution:
\begin{center}
\begin{tabular}[c]{c c}
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{job name}}$\texttt{.p}$\sbrkt{\textit{i}}$\texttt{.out}}
&\parbox[b]{3.0in}{``echo'' file of parameters file}\\
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{job name}}$\texttt{.p}$\sbrkt{\textit{i}}$\texttt{.log}}
&\parbox[b]{3.0in}{message passing log file}\\
 \parbox[b]{3.0in}{\raggedleft \texttt{console}$\sbrkt{\textit{i}}$}
&\parbox[b]{3.0in}{redirected console output}\\
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{job name}}$\texttt{.io}$\sbrkt{\textit{j}}$\texttt{.exo}}
&\parbox[b]{3.0in}{results data (\textsf{ExodusII}~\cite{ExodusII})}
\end{tabular}
\end{center}
where $j = 0,\ldots,\rbrkt{\textit{number of output groups} - 1}$.
The output from the
standard output streams is not redirected to a file for the processor with
rank 0, that is, no \texttt{console0} file is created.  Any prompts for use input
still appear on the screen.  When running a job in the background the user
may opt to redirect the screen output to a file, called \texttt{console0} for
consistency.  The following input makes use of the \texttt{-f} command line option
to specify the name of the parameters file:
\begin{center}
\texttt{\% mpirun -np} $\sbrkt{\textit{number of processors}}$ \texttt{tahoe -f}
$\sbrkt{\textit{job name}}$\texttt{.in \textgreater~console0 \&}
\end{center}
On non-limited memory machines, the output from the distributed output
groups is collected on processors as designated in the output map file. 
Since the output is ``joined'' at run time, no post-processing of the results
data is required.  Running \tahoe in this mode does require more memory. 
\tahoe will leave the output in its decomposed form if launched with the
\texttt{-split\_io} command line option:
\begin{center}
\texttt{\% mpirun -np} $\sbrkt{\textit{number of processors}}$ \texttt{tahoe -split\_io}
\end{center}
When running on a limited memory machine, or when specifying the 
\texttt{-split\_io}
option, \tahoe will generate distributed results files:
\begin{center}
\begin{tabular}[c]{c c}
 \parbox[b]{3.0in}{\raggedleft $\sbrkt{\textit{job name}}$\texttt{.p}$\sbrkt{\textit{i}}$\texttt{.io}$\sbrkt{\textit{j}}$\texttt{.exo}}
&\parbox[b]{3.0in}{distributed results file}
\end{tabular}
\end{center}
where $i = 0,\ldots,\rbrkt{\textit{number of partitions} - 1}$ and 
$j = 0,\ldots,\rbrkt{\textit{number of output groups} - 1}$.
The distributed results files must then be joined in a
separate post-processing step to produce the results files that would have
been written by a serial execution of the same calculation.

\subsection{Domain decomposition}
\label{sect.domain.decomp}
Serial mesh decomposition of a large mesh often requires more memory than
some architectures have for each individual compute node.  In these cases
the user must perform the decomposition and results file joining on a
pre/post processor machine that is not memory limited.  If \tahoe is
launched with the -decomp command line option, it will generated the
decomposition files without subsequently performing any analysis:
\begin{center}
\texttt{\% tahoe -decomp}
\end{center}
As mentioned about, \tahoe creates a domain decomposition based on the
connectivities seen \emph{at run time}.  This is a combination of information in
the parameters and geometry files.  If the geometry file contains blocks of
elements that are not referenced in the parameters file, these blocks are
not considered during decomposition.  Also, \tahoe does not record the
values in the parameters files used to generate a given decomposition.  If
the parameters file changes in any way that would affect the set of
``active'' elements, or the connectivities active at run time, the user must
remove any existing decomposition files.  Otherwise, \tahoe will use any
existing decomposition files.

The decomposition process is not parallelized.  In order to avoid
allocating processors that will remain idle, \tahoe will not perform
decomposition on a parallel platform if more than one processors is
allocated for execution.  Therefore, decomposition must be initiated 
with:
\begin{center}
\texttt{\% mpirun -np 1 tahoe -decomp}
\end{center}
After prompting the user for the name of the job file, \tahoe will prompt
the user for the number of partitions to create.

\subsection{Joining results files}
\label{sect.joining}
To join distributed output files, the parameters, geometry, decomposition
files, and distributed results files are needed.  As with the decomposition
process, the joining process has not been parallelized.  On a
multi-processor platform, it must be launched with a single process:
\begin{center}
\texttt{\% mpirun -np 1 tahoe -join}
\end{center}
After prompting the user for the name of the job file, \tahoe will prompt
the user for the number of partitions in the distributed results data.  The
number given to \tahoe must be the \emph{total} number of partitions generated from
the original model geometry.  Under certain circumstances, not all
partitions will contribute distributed output to all output groups.
